{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Développement et déploiement d'un projet d'analyse de sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I) Le projet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Air Paradis* souhaite pouvoir disposer d'un **produit IA** permettant d'**anticiper les bad buzz** sur les **réseaux sociaux**.\n",
    "\n",
    "Le projet a alors consisté à :\n",
    "- charger un jeu de données contenant des tweets étiquettés \n",
    "- réaliser l'**analyse exploratoire (*EDA*)** sur ce jeu de données pour décider quels traitements seraient opportuns\n",
    "- mettre en oeuvre le **preprocessing** adéquat et surtout adapté au modèle testé\n",
    "- tester différents modèles et les optimiser\n",
    "- sélectionner un modèle gagnant\n",
    "- comprendre le modèle grâce à l'**interprétabilité**\n",
    "- rendre disponible ce modèle via une **API**\n",
    "- et créer une **UI** permettant d'y accéder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II) La démarche MLOps mise en oeuvre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'autre facette du projet consistait à le construire en suivant les concepts d'une démarche **MLops**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1) Qu'est-ce que le *MLOps* ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"MLOPS_diagram.png\" alt=\"MLOPS_diagram\" width=\"1000\" class=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLOps** est un ensemble de pratiques qui **combine** les techniques de développement logiciel (**DevOps**) et d’apprentissage automatique (**Machine Learning**) pour optimiser et automatiser le cycle de vie de l’apprentissage machine. Le but de MLOps est d’aider les organisations à **construire, déployer et gérer** des modèles d’apprentissage machine en environnement de production **plus efficacement et plus rapidement**. \n",
    "\n",
    "Voici quelques principes clés du MLOps :\n",
    "- Développement du **pipeline de traitement des données** :\n",
    "    - cela comprend le travail préliminaire de collecte des données, d'exploration, etc.\n",
    "    - nettoyage, traitements, feature engineering, etc\n",
    "- Développement du **pipeline de modélisation** :\n",
    "    - entraînement de différents modèles, avec différentes combinaisons d'hyperparamètres\n",
    "    - évaluation et sélection du meilleur modèle\n",
    "- **Intégration continue** du pipeline général :\n",
    "    - utilisation d'une platefrome de développement collaboratif et de gestion de version\n",
    "    - pour travailler sur le code source et suivre ses modifications\n",
    "    - tests unitaires, tests de performance, etc.\n",
    "- **Déploiement continue** du pipeline général :\n",
    "    - automatisation totale du pipeline\n",
    "    - y compris collecte, préparation, recherche d'hyperparamètres\n",
    "- **Tracking** :\n",
    "    - utilisation de solutions (comme `MLflow`) pour faire un suivi automatisé des différents expérimentations conduites avec le pipeline\n",
    "    - enregistrement des paramètres de préparation, du type de modèle, des hyperparamètres et enfin des mesures d'évaluation associées\n",
    "- **Enregistrement des modèles** :\n",
    "    - chaque version du modèle est consignée\n",
    "    - avec toutes les données/informations ayant servi à la construire\n",
    "- **Déploiement continue du modèle**\n",
    "    - mettre en place un serveur ou utiliser une plateform cloud\n",
    "    - y installer les dépendances nécessaires pour utiliser le modèle\n",
    "    - créer une API permettant d'accéder au modèle et d'optenir des prédictions\n",
    "- **Monitoring** :\n",
    "    - suivi des performances du modèle en production\n",
    "    - envoi d'alertes si le modèle n'est plus aussi bon sur les données en temps réel\n",
    "- **Entraînement continue** :\n",
    "    - en cas de dégradation de la performance (alerte)\n",
    "    - mise à jour du modèle en utilisant le pipeline général automatisé\n",
    "- **Gouvernance et Conformité** :\n",
    "    - respecter les règlements et exigences relatifs à la confidentialité des données,\n",
    "    - ainsi qu'à la sécurité et à l’utilisation éthique de l’IA\n",
    "- **Scalabilité** :\n",
    "    - absorber les évolutions de la demande\n",
    "    - système distribué pour permettre les mutations des solutions logiciels et matériels \n",
    "- **Collaboration et documentation** :\n",
    "    - documenter l'ensemble du cycle de vie du produit IA (le code, la donnée, l'architecture)\n",
    "    - communiquer / évangéliser pour améliorer les performances et détecter les problèmes\n",
    " \n",
    "Pour chacune de ces étapes, des solutions logiciels existent et permettent d'implémenter des projets IA avec une meilleure efficacité ainsi qu'une plus grand fiabilité. Ils ne sont pas tous obligatoires et dépendent des besoins de chaque projet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2) Et pour notre cas ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour notre projet nous avons appliqué certaines facettes du *MLOps* :\n",
    "- utilisation de **`MLFlow`** pour :\n",
    "    - tracker les hyperparamètres de nos différents modèles\n",
    "    - tracker les métriques\n",
    "    - enregistrer les modèles que nous souhaitons conserver\n",
    "    - charger le modèle gagnant et faire des prédictions\n",
    "    - tester le service de *serving*\n",
    "- mis en oeuvre de **pipelines de preprocessing et/ou apprentissage** suivant les cas\n",
    "- utilisation des ces pipelines pour **optimiser les hyperparamètres** avec **`Optuna`**\n",
    "- **intégration continue du code** avec **`GitHub`**\n",
    "- mis en oeuvre d'un **pipeline de déploiement continu** de l'API et de l'UI (grâce à **AZURE** et **GitHub Actions**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III) Le preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le preprocessing, nous avions plusieurs cas :\n",
    "- les **opérations générales** pouvant être appliquées aussi bien pour notre modèle simple (basé sur une vectorisation de type bag-of-words) que pour notre modèle avancé (basé sur une vectorisation de type word embedding) :\n",
    "    - mettre en minuscule\n",
    "    - retirer les codes HTML correspondants à des caractères spéciaux\n",
    "    - remplacer (par \"url\" par exemple) les liens hypertextes\n",
    "    - remplacer (par \"mail\" par exemple) les adresses mails\n",
    "    - retirer les séquences d'échappement\n",
    "    - corriger les mots avec 3+ lettres répétées\n",
    "    - corriger les contractions\n",
    "    - remplacer les emoticons avec leur sens\n",
    "    - remplacer (par \"hashtag\" par exemple) les #xxxx\n",
    "    - remplacer (par \"mention\" par exemple) les @xxxx\n",
    "    - enfin appliquer les méthode `strip`\n",
    "- les opérations **plus spécifiques** à un modèle :\n",
    "    - **modèle simple Machine Learning** :\n",
    "        - mettre en minuscule (si ce n'est pas déjà fait)\n",
    "        - retirer les stopwords\n",
    "        - retirer d'éventuels stopwords personnalisés\n",
    "        - retirer la ponctuation\n",
    "        - retirer les espaces en trop\n",
    "        - retirer les nombres et affiliés (exemple : \"nine\")\n",
    "        - filtrer sur les Part-of-Speech tags (exemple : ne garder que les adjectifs)\n",
    "        - normaliser via stemmatisation ou lemmatisation\n",
    "    - **modèle avancé Deep Learning** :\n",
    "        - mettre en minuscule (si ce n'est pas déjà fait)\n",
    "        - retirer la ponctuation\n",
    "        - retirer les espaces en trop\n",
    "        - normaliser via stemmatisation ou lemmatisation\n",
    "\n",
    "            Ces opérations sont plus légères que pour le modèle simple, car nous utilisons pour le modèle avancé un *LSTM*, un réseau de neurones récurrents capable de prendre en compte les **relations de dépendance entre les éléments successifs d'une séquence**. Il est donc par exemple important de conserver les stop words car le réseau sera capable de détecter la différence entre *happy* et *not happy*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV) Les modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.0) La démarche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV.0.1) Séparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De manière classique nous avons séparé nos données en un jeu d'entraînement `X_train, y_train`et un jeu de test `X_test, y_test`.\n",
    "\n",
    "Pour comparer les modèles issus de la recherche des meilleurs hyperparamètres, nous avons utilisé :\n",
    "- une **validation croisée** pour le **modèle simple** \n",
    "- un split **train --> train/validation** pour le **modèle avancé** (eu égard aux temps d'entraînement bien plus conséquents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV.0.2) Quelle métrique d'évaluation ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Air Paradis* cherche à anticiper les bad buzz, **la classe qui nous intéresse est donc *1 - Negative*** (pour rappel nous avons ré-attribué les étiquettes : 0 --> 1 et 4 --> 0). \n",
    "\n",
    "Nous avons la chance d'avoir un jeu de données **équilibré**, nous permettant de rester classique sur le choix de la métrique d'évaluation :\n",
    "\n",
    "- Nous pourrions nous intéresser seulement à l'*accuracy*, mais cela reviendrait à choisir à la place du client un seuil de décision égale 0.5.\n",
    "- Nous allons donc **chercher à optimiser la *ROC AUC***, qui nous permettra dans cette première phase du projet de sélectionner le modèle le plus performant tout en restant polyvalent. *Air Paradis* pourra par la suite travailler avec nous pour déterminer le seuil le plus pertinent pour son besoin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons **enregistré** également l'*accuracy* et le *training time*, et avons **tracé** les courbes *ROC* et *Precision-Recall*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.1) Modèle classique de machine learning - Régression Logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le modèle simple, nous avons :\n",
    "- créé un **pipeline d'apprentissage** pour un modèle scikit learn dans le cadre d'une validation croisée, avec le tracking offert par `MLFlow`\n",
    "- créé des classes `Scikit Learn` intégrant toutes les opération de nettoyages, afin de pouvoir êtres intégrées dans une `pipeline` `Scikit Learn`\n",
    "- utilisé le `TfidfVectorizer` pour représenter notre corpus de façon numérique et utilisant la fréquence des mots à l'échelle des documents, **mais aussi à l'échelle du corpus**, afin de **réduire l'impact des mots trop fréquents**\n",
    "- créé une fonction de recherche d'hyperparamètres basée sur `Optuna`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V.1.1) *Dummy*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./mySaves/plots/simple_0_dummy.png\" alt=\"simple_0_dummy\" width=\"800\" class=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V.1.2) *Logistic regression* - Premier essai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./mySaves/plots/simple_1_first_try.png\" alt=\"simple_1_first_try\" width=\"800\" class=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V.1.3) *Logistic regression* - Optuna\n",
    "\n",
    "Utilisation d'`Optuna` pour trouver les meilleurs hyperparamètres :\n",
    "- pour le `simpleModelCleaner` \n",
    "- puis pour `LogisticRegression`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./mySaves/plots/simple_2_optuna_clean.png\" alt=\"simple_2_optuna_clean\" width=\"800\" class=\"center\"/>\n",
    "<img src=\"./mySaves/plots/simple_3_optuna_LR.png\" alt=\"simple_3_optuna_LR\" width=\"800\" class=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V.1.4) *Logistic regression* - Modèle final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suite aux différentes étapes réalisées pour la construction de notre modèle simple, nous pouvons apprécier l'évolution des résultats sur la CV :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./mySaves/plots/simple_4_bilan.png\" alt=\"simple_4_bilan\" width=\"800\" class=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons ainsi contruire notre modèle simple final :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./mySaves/vscode_captures/simple_pipeline_sklearn.png\" alt=\"simple_pipeline_sklearn\" width=\"2000\" class=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et l'utiliser sur notre set de test :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./mySaves/plots/simple_5_final.png\" alt=\"simple_5_final\" width=\"800\" class=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.2) Modèle avancé de deep learning - Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le modèle avancé nous avons utilisé :\n",
    "- Une couche `Text_vectorization` :\n",
    "    - nettoyer préalablement nos tweets via l'argument `standardize` : nous permettant d'appliquer différentes combinaisons d'opérations (décrites ci-dessus dans *III) Preprocessing*)\n",
    "    - transformer notre Series de tweets **en une liste de séquences d'indices de tokens** (via l'argument `output_mod = \"int\"`). Le nombre d'indices disponible est **borné par `max_tokens`, qui est la taille maximale** du dictionnaire\n",
    "    - appliquer un *padding*, c'est-à-dire remplir les séquences avec des \" \"  (ou les tronquer si elles sont trop longues) afin qu'elles atteignent un longueur donnée (`output_sequence_length`).\n",
    "- Une **couche d'embedding**, qui crée des plongements de mots :\n",
    "    - Elle prend en entrée des **séquences d'entiers** issues de la vectorisation\n",
    "    - Elle a pour arguments `input_dim` qui correspond au nombre de mots uniques dans le vocabulaire (déterminé lors de la vectorisation) ainsi que `output_dim` qui correspond à la dimension des vecteurs de mots\n",
    "    - Elle sort alors un tensor de dimensions **nb de séquences $*$ longueur des séquence $*$ dimension des vecteurs de mot**\n",
    "    - Nous pouvons initialiser les poids de cette couche d'embedding grâce un embedding **pré-entraîné**, pour cela nous avons **créé une fonction** permettant de créer cette matrice de poids pré-entraînés à partir d'un fichier téléchargé sur internet : `get_embedding_matrix`.\n",
    "- Une **couche LSMT (Long Short-Term Memory) bidirectionnelle**, qui permet de dédoubler les unités *LSTM* afin de parcourir les séquences de gauche à droite (sens de la lecture) et de droite à gauche, permettant ainsi de **capter des dépendances dans les 2 sens**. Pour cette couche nous pouvons faire varier :\n",
    "    - `units` : qui correspond au nombre d'unités *LSTM* dans chacune des 2 directions\n",
    "    - `dropout` : qui correspond à la fraction d'unités *LSTM* qui seront déconnectées à chaque étape de la lecture des séquences\n",
    "    - `recurrent_dropout` : qui correspond à la fraction d'unités *LSTM* qui se seront déconnectées à chaque étape de la lecture de la cellule mémoire\n",
    "- une **dense** à 16 unités avec une fonction d'activation **relu** (pour ajouter de la non-linéarité)\n",
    "- une **dense** à une seule unité (une seule classe à prédire) avec une fonction d'activation **sigmoïde** (pour obtenir un score/une probabilité entre 0 et 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La création et l'entraînement de ce modèle a été intégré à différents pipelines d'apprentissage, au sein de *runs* `MLFlow` pour le tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V.2.1) *Baseline*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons utilisé un modèle **non récurrent** (pas de couche LSTM mais un Flatten) comme base de comparaison :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./mySaves/plots/advanced_0_baseline.png\" alt=\"advanced_0_baseline\" width=\"800\" class=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V.2.2) Premier essai de notre model `LSTM` bidirectionnel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons d'abord testé notre modèle **sans normalisation** et **sans matrice d'embedding** pré-entraîné :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./mySaves/plots/advanced_1_first_try.png\" alt=\"advanced_1_first_try\" width=\"800\" class=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V.2.3) **`normalization = \"stem\"`** ou **`normalization = \"lem\"`** ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis nous avons testé la lemmatisation et la stemmatisation :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./mySaves/plots/advanced_2_stem.png\" alt=\"advanced_2_stem\" width=\"800\" class=\"center\"/>\n",
    "<img src=\"./mySaves/plots/advanced_3_lem.png\" alt=\"advanced_3_lem\" width=\"800\" class=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons choisi la lemmatisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V.2.4) *Quel embedding* ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons ensuite comparé **GLOVE** et **FASTTEXT**, **entraînable** ou **pas** :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./mySaves/vscode_captures/advanced_embedding_results.png\" alt=\"advanced_embedding_results\" width=\"1000\" class=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons choisi GLOVE avec une couche entraînable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V.2.5) Quels hyperparamètres pour le modèle `LSTM` ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois choisi les hyperparamètres des étapes préliminaires de notre pipeline, nous nous sommes intéressés :\n",
    "- au modèle `LSTM` (`LSTM_units`, `LSTM_dropout`, `LSTM_recurrent_dropout`, `optimizer`)\n",
    "- aux paramètres d'entraînement (`early_stopping_patience`, `batch_size`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./mySaves/mlflow_ui_plots/advanced/6_optuna_26_best.png\" alt=\"6_optuna_26_best\" width=\"2000\" class=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V.2.6) Modèle avancé final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suite aux différentes étapes réalisées pour la construction de notre modèle avancé, nous pouvons apprécier l'évolution des résultats sur le set de validation :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./mySaves/plots/advanced_9_bilan.png\" alt=\"advanced_9_bilan\" width=\"800\" class=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons ainsi contruire notre modèle avancé final :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./mySaves/vscode_captures/advanced_final.png\" alt=\"advanced_final\" width=\"800\" class=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et l'utiliser sur notre set de test :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./mySaves/plots/advanced_10_final.png\" alt=\"advanced_10_final\" width=\"800\" class=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.3) Test de l'apport d'un modèle BERT ou d'un embedding USE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le  cadre du projet, nous avons également testé des modèles basés sur **BERT** (*Bidirectional Encoder Representations from Transformers*) et sur **USE** (*Universal Sentence Encoder*), avec des résultats très intéressants en termes de **performance** :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./mySaves/plots/COMPARISON_ON_TEST.png\" alt=\"COMPARISON_ON_TEST\" width=\"800\" class=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V) Interprétabilité du modèle avancé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons étudier l'interprétabilité du modèle avancé afin de rendre moins opaque le processus de décision du modèle. Nous avons utilisé la libraire `SHAP` qui utilise des approximations des valeurs de Shapley et est une des méthodes d'explicabilité les plus utilisées. L'objectif est de connaître la contribution de chaque feature à une prédiction donnée, ou plus exactement : comment chaque feature fait différer une prédiction de la prédiction moyenne.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.1) Interprétabilité globale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./mySaves/shap/global_imp.png\" alt=\"global_imp\" width=\"1200\" class=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.2) Interprétabilité locale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./mySaves/shap/force_plot_0.1.png\" alt=\"force_plot_0.1\" width=\"1000\" class=\"center\"/>\n",
    "<img src=\"./mySaves/shap/force_plot_0.2.png\" alt=\"force_plot_0.3\" width=\"1000\" class=\"center\"/>\n",
    "<img src=\"./mySaves/shap/force_plot_0.3.png\" alt=\"force_plot_0.3\" width=\"1000\" class=\"center\"/>\n",
    "<img src=\"./mySaves/shap/force_plot_1.1.png\" alt=\"force_plot_1.1\" width=\"1000\" class=\"center\"/>\n",
    "<img src=\"./mySaves/shap/force_plot_1.2.png\" alt=\"force_plot_1.2\" width=\"1000\" class=\"center\"/>\n",
    "<img src=\"./mySaves/shap/force_plot_1.3.png\" alt=\"force_plot_1.3\" width=\"1000\" class=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI) Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons pu explorer différentes approches de modélisation pour répondre à la problématique du projet. Nous avons optimisé deux modèles : Un modèle simple basé sur une régression linéaire et un modèle avancé basé sur un LSTM bidirectionnel. Des modèles basés sur BERT et USE ont également été testés. Ces derniers ont donné des résultats très prometteurs, sans avoir été optimisés. Explorer plus de modèles pré-trainés basés sur des transformers nous permttrait sans doute de gagner en performance, voir même en interprétabilité grâce à des librairies spécialisées comme `transformers_interpret`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envProject7",
   "language": "python",
   "name": "envproject7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
